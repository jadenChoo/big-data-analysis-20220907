{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f343c661",
   "metadata": {},
   "source": [
    "## Feature Extraction - English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ff1ce",
   "metadata": {},
   "source": [
    "추준호(20224224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e7009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653e4f1",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758bd2e",
   "metadata": {},
   "source": [
    "### (1) nltk & gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a92fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf6b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    \n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "            \n",
    "        yield stemmer.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c31e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elephant sneezed at the sight of potatoes.\n",
      "<generator object tokenize at 0x177ec0e40>\n",
      "\n",
      "Bats can see via echolocation. See the bat sight sneeze!\n",
      "<generator object tokenize at 0x177ec06d0>\n",
      "\n",
      "Wondering, she opened the door to the studio.\n",
      "<generator object tokenize at 0x177ec0e40>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus :\n",
    "    print(doc)\n",
    "    \n",
    "    tokens = tokenize(doc)\n",
    "    print(tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ce5e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elephant sneezed at the sight of potatoes.\n",
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "\n",
      "Bats can see via echolocation. See the bat sight sneeze!\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "\n",
      "Wondering, she opened the door to the studio.\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus :\n",
    "    print(doc)\n",
    "    \n",
    "    tokens = list(tokenize(doc))\n",
    "    print(tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "704d2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [\n",
    "    list(tokenize(doc)) for doc in corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059f9224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato'],\n",
       " ['bat',\n",
       "  'can',\n",
       "  'see',\n",
       "  'via',\n",
       "  'echoloc',\n",
       "  'see',\n",
       "  'the',\n",
       "  'bat',\n",
       "  'sight',\n",
       "  'sneez'],\n",
       " ['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbaeea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = gensim.corpora.Dictionary(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4474f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'at')\n",
      "(1, 'eleph')\n",
      "(2, 'of')\n",
      "(3, 'potato')\n",
      "(4, 'sight')\n",
      "(5, 'sneez')\n",
      "(6, 'the')\n",
      "(7, 'bat')\n",
      "(8, 'can')\n",
      "(9, 'echoloc')\n",
      "(10, 'see')\n",
      "(11, 'via')\n",
      "(12, 'door')\n",
      "(13, 'open')\n",
      "(14, 'she')\n",
      "(15, 'studio')\n",
      "(16, 'to')\n",
      "(17, 'wonder')\n"
     ]
    }
   ],
   "source": [
    "for x in lexicon.items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a8a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)]\n",
      "\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "[(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)]\n",
      "\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n",
      "[(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    print(doc)\n",
    "    \n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    print(vec)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7f376",
   "metadata": {},
   "source": [
    "## (2) scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3169f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e7f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be65a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1834365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 2 0 0 0]\n",
      " [0 1 1 1 0 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(results.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4533cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 16,\n",
       " 'elephant': 6,\n",
       " 'sneezed': 14,\n",
       " 'at': 0,\n",
       " 'sight': 12,\n",
       " 'of': 7,\n",
       " 'potatoes': 9,\n",
       " 'bats': 2,\n",
       " 'can': 3,\n",
       " 'see': 10,\n",
       " 'via': 18,\n",
       " 'echolocation': 5,\n",
       " 'bat': 1,\n",
       " 'sneeze': 13,\n",
       " 'wondering': 19,\n",
       " 'she': 11,\n",
       " 'opened': 8,\n",
       " 'door': 4,\n",
       " 'to': 17,\n",
       " 'studio': 15}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5153a1",
   "metadata": {},
   "source": [
    "# Compare nltk & sklern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac563a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'elephant', 'sneezed', 'at', 'sight', 'of', 'potatoes', 'bats', 'can', 'see', 'via', 'echolocation', 'bat', 'sneeze', 'wondering', 'she', 'opened', 'door', 'to', 'studio'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f71649ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'eleph',\n",
       " 'of',\n",
       " 'potato',\n",
       " 'sight',\n",
       " 'sneez',\n",
       " 'the',\n",
       " 'bat',\n",
       " 'can',\n",
       " 'echoloc',\n",
       " 'see',\n",
       " 'via',\n",
       " 'door',\n",
       " 'open',\n",
       " 'she',\n",
       " 'studio',\n",
       " 'to',\n",
       " 'wonder']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lexicon.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ed80c",
   "metadata": {},
   "source": [
    "#### NLTK only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b545f249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'echoloc', 'eleph', 'open', 'potato', 'sneez', 'wonder'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(lexicon.values()) - set(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569d34d",
   "metadata": {},
   "source": [
    "#### SKLearn only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4987e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bats',\n",
       " 'echolocation',\n",
       " 'elephant',\n",
       " 'opened',\n",
       " 'potatoes',\n",
       " 'sneeze',\n",
       " 'sneezed',\n",
       " 'wondering'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(vectorizer.vocabulary_.keys()) - set(lexicon.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941d913",
   "metadata": {},
   "source": [
    "## 2. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09072b76",
   "metadata": {},
   "source": [
    "### (1) Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af98ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)]\n",
      "[(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    \n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1078b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "[(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = [(x[0],1) for x in vec]\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521e44d",
   "metadata": {},
   "source": [
    "### (2) Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b857ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bdc3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "745ef59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c47bc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94c45b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = onehot.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "922be465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b3953",
   "metadata": {},
   "source": [
    "# 3. Tf-Idf(Term frequency to Inverse document frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e424c0e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    tf (t, d) &= 1 + \\log \\, f_{t,d}  \\\\\n",
    "    idf (t,D) &= \\log 1 + \\frac{N}{n_t} \\\\\n",
    "    tf-idf (t, d, D) &= tf (t,d) \\cdot idf (t,D)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adf7e8",
   "metadata": {},
   "source": [
    "## (1) Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b66d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fe61275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4837965208957426), (1, 0.4837965208957426), (2, 0.4837965208957426), (3, 0.4837965208957426), (4, 0.17855490118826325), (5, 0.17855490118826325)]\n",
      "[(4, 0.10992597952954358), (5, 0.10992597952954358), (7, 0.5956913654963344), (8, 0.2978456827481672), (9, 0.2978456827481672), (10, 0.5956913654963344), (11, 0.2978456827481672)]\n",
      "[(12, 0.408248290463863), (13, 0.408248290463863), (14, 0.408248290463863), (15, 0.408248290463863), (16, 0.408248290463863), (17, 0.408248290463863)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = tfidf[vec]\n",
    "    \n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59f83ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('at', 0.4837965208957426), ('eleph', 0.4837965208957426), ('of', 0.4837965208957426), ('potato', 0.4837965208957426), ('sight', 0.17855490118826325), ('sneez', 0.17855490118826325)]\n",
      "[('sight', 0.10992597952954358), ('sneez', 0.10992597952954358), ('bat', 0.5956913654963344), ('can', 0.2978456827481672), ('echoloc', 0.2978456827481672), ('see', 0.5956913654963344), ('via', 0.2978456827481672)]\n",
      "[('door', 0.408248290463863), ('open', 0.408248290463863), ('she', 0.408248290463863), ('studio', 0.408248290463863), ('to', 0.408248290463863), ('wonder', 0.408248290463863)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = tfidf[vec]\n",
    "    \n",
    "    vec = [(tfidf.id2word[x[0]], x[1]) for x in vec]\n",
    "    \n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d007ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "[('at', 0.4837965208957426), ('eleph', 0.4837965208957426), ('of', 0.4837965208957426), ('potato', 0.4837965208957426), ('sight', 0.17855490118826325), ('sneez', 0.17855490118826325)]\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "[('sight', 0.10992597952954358), ('sneez', 0.10992597952954358), ('bat', 0.5956913654963344), ('can', 0.2978456827481672), ('echoloc', 0.2978456827481672), ('see', 0.5956913654963344), ('via', 0.2978456827481672)]\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n",
      "[('door', 0.408248290463863), ('open', 0.408248290463863), ('she', 0.408248290463863), ('studio', 0.408248290463863), ('to', 0.408248290463863), ('wonder', 0.408248290463863)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    print(doc)\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = tfidf[vec]\n",
    "    \n",
    "    vec = [(tfidf.id2word[x[0]], x[1]) for x in vec]\n",
    "    \n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfea4f",
   "metadata": {},
   "source": [
    "## (2) Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8106c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a28261e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32dfd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14ae023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37867627, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.37867627, 0.37867627, 0.        , 0.37867627,\n",
       "        0.        , 0.        , 0.28799306, 0.        , 0.37867627,\n",
       "        0.        , 0.44730461, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.30251368, 0.30251368, 0.30251368, 0.        ,\n",
       "        0.30251368, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.60502736, 0.        , 0.23006945, 0.30251368, 0.        ,\n",
       "        0.        , 0.17866945, 0.        , 0.30251368, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.36772387,\n",
       "        0.        , 0.        , 0.        , 0.36772387, 0.        ,\n",
       "        0.        , 0.36772387, 0.        , 0.        , 0.        ,\n",
       "        0.36772387, 0.43436728, 0.36772387, 0.        , 0.36772387]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa4501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73da590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc294df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272addf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286e1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd6e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
